{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "eb0bfa44",
   "metadata": {},
   "source": [
    "## 1. 뉴스제목 가져오기\n",
    "* user-agent 요청헤더를 반드시 설정해야 한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0d27781",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name: requests\n",
      "Version: 2.32.3\n",
      "Summary: Python HTTP for Humans.\n",
      "Home-page: https://requests.readthedocs.io\n",
      "Author: Kenneth Reitz\n",
      "Author-email: me@kennethreitz.org\n",
      "License: Apache-2.0\n",
      "Location: C:\\Users\\winds\\anaconda3\\Lib\\site-packages\n",
      "Requires: certifi, charset-normalizer, idna, urllib3\n",
      "Required-by: anaconda-auth, anaconda-catalogs, anaconda-client, anaconda-project, conda, conda-build, conda-repo-cli, conda_package_streaming, cookiecutter, datashader, jupyterlab_server, panel, PyGithub, requests-file, requests-toolbelt, Sphinx, streamlit, tldextract\n"
     ]
    }
   ],
   "source": [
    "# requests 라이브러리 설치여부 확인\n",
    "\n",
    "!pip show requests \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f7798ea8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name: beautifulsoup4\n",
      "Version: 4.12.3\n",
      "Summary: Screen-scraping library\n",
      "Home-page: https://www.crummy.com/software/BeautifulSoup/bs4/\n",
      "Author: \n",
      "Author-email: Leonard Richardson <leonardr@segfault.org>\n",
      "License: MIT License\n",
      "Location: C:\\Users\\winds\\anaconda3\\Lib\\site-packages\n",
      "Requires: soupsieve\n",
      "Required-by: conda-build, nbconvert\n"
     ]
    }
   ],
   "source": [
    "# beautifulsoup4 라이브러리 설치여부 확인\n",
    "!pip show beautifulsoup4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "bf80a429",
   "metadata": {},
   "outputs": [],
   "source": [
    "# reqeusts, bs4 import\n",
    "import requests  # requests 라이브러리 import\n",
    "import bs4  # BeautifulSoup 라이브러리 import\n",
    "\n",
    "# BeautifulSoup 클래스 import\n",
    "from bs4 import BeautifulSoup  # BeautifulSoup 클래스 import\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "9788d6ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "requests version: 2.32.3\n",
      "bs4 version: 4.12.3\n"
     ]
    }
   ],
   "source": [
    "# requests, bs4 버전 확인하기\n",
    "\n",
    "print(f\"requests version: { requests.__version__}\")\n",
    "print(f\"bs4 version: { bs4.__version__}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccb91def-cde3-4aeb-b0e5-2e7233500333",
   "metadata": {},
   "source": [
    "### 1. 뉴스 제목 추출하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "94c12fd6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://news.naver.com/section/105\n",
      "\n",
      "UNIST, '전기 신호 만드는 접착 필름 개발'\n",
      "\n",
      "\n",
      "\n",
      "유진투자증권 ‘AI자동투자&중소형주 투자전략’ 세미나 개최\n",
      "\n",
      "\n",
      "\n",
      "통신 3사, ‘갤럭시Z 폴드·플립7′, 사전 예약 고객 개통 시작… 다양한 프로모션 제공\n",
      "\n",
      "\n",
      "\n",
      "영상 시리즈 선보이자…웹툰 '파인' 조회수 58배, 매출 26배 뛰었다\n",
      "\n",
      "\n",
      "\n",
      "건설연, 교량 안전 자체진단 케이블 모니터링 시스템 개발\n",
      "\n",
      "\n",
      "\n",
      "“7년간 북극 온난화 실험결과”…기후변화 미래경로 예측 단서 찾았다\n",
      "\n",
      "\n",
      "\n",
      "국제수학올림피아드서 한국대표단 전원 메달 받아\n",
      "\n",
      "\n",
      "\n",
      "지질자원연-동티모르, 11년만 협력 재개\n",
      "\n",
      "\n",
      "\n",
      "단통법 드디어 폐지…하지만 ‘호갱님’ 우려는 계속 [뉴스in뉴스]\n",
      "\n",
      "\n",
      "\n",
      "넷플릭스·디즈니, 영상 제작에 AI 활용 추진…일자리 위협 우려 ↑\n",
      "\n",
      "\n",
      "\n",
      "삼성SDS·LG CNS·SK AX…AI판 키운다, 2분기 실적도 '순풍'\n",
      "\n",
      "\n",
      "\n",
      "[IT클로즈업] 공공 사업 확장 승부수…폴라리스오피스, 공공SW 강자 핸디소프트 인수\n",
      "\n",
      "\n",
      "\n",
      "피부 냄새 맡은 개가 파킨슨병 찾아낸다\n",
      "\n",
      "\n",
      "\n",
      "국가대표 AI, 네이버 vs LG ‘양강’ 구도?…특허 출원량 1위는 KT\n",
      "\n",
      "\n",
      "\n",
      "이통3사 갤럭시Z7 지원금, 최대 60만원…단통법 폐지 전보다 10만원↑\n",
      "\n",
      "\n",
      "\n",
      "불법 스테로이드 제조·판매한 母子 적발…약사법 등 위반 혐의(종합)\n",
      "\n",
      "\n",
      "\n",
      "'겸배' 혹사에 직내괴 호소까지…경기 집배원 극단선택 시도\n",
      "\n",
      "\n",
      "\n",
      "\"가격이 문제가 아녔어~\"…플립보다 많이 팔린 폴드\n",
      "\n",
      "\n",
      "\n",
      "100만원짜리 폰이 공짜?… 단통법 폐지, 시작된 ‘보조금 전쟁’\n",
      "\n",
      "\n",
      "\n",
      "이더리움 주도 '가상자산 불장', 주요 알트코인까지 상승세 확산\n",
      "\n",
      "\n",
      "\n",
      "한반도 극한 폭우 시기, 8월에서 7월로 빨라진다\n",
      "\n",
      "\n",
      "\n",
      "국가 AI 핵심자원 ‘GPU 확보 사업’ 선정 임박…당락 가를 관건은?\n",
      "\n",
      "\n",
      "\n",
      "\"아따, 뭔 또 시비여\" 디즈니+ 촌뜨기들 활약에…'파인' 웹툰 흥행 역주행\n",
      "\n",
      "\n",
      "\n",
      "단통법, 오늘 역사속으로…與 \"소비자 중심 이통시장 만들어야\"\n",
      "\n",
      "\n",
      "\n",
      "한번도 본적 없는 감각기관 가진 북극 신종 물곰\n",
      "\n",
      "\n",
      "\n",
      "[콘텐츠뷰] '아이쇼핑', 불법 입양의 민낯을 꼬집다\n",
      "\n",
      "\n",
      "\n",
      "단통법 폐지 하루 앞둔 신도림, 번호이동 전쟁에 '공짜폰' 속출[르포]\n",
      "\n",
      "\n",
      "\n",
      "크롬 브라우저 없는 아이폰·갤럭시폰 나올까\n",
      "\n",
      "\n",
      "\n",
      "[굿바이, 단통법]① 단통법의 시작, 어떻게 탄생했나\n",
      "\n",
      "\n",
      "\n",
      "韓 금융기관 강타한 랜섬웨어 조직, 한달전 두바이 종합병원까지 털었다\n",
      "\n",
      "\n",
      "\n",
      "​​​​​KT, '갤럭시 Z 폴드·플립7' 사전 개통 시작…출고가 50% 할인 '미리보상' 인기\n",
      "\n",
      "\n",
      "\n",
      "[단독]배민 이어 쿠팡이츠·요기요도 '한그릇' 마케팅…제2의 무료배달?\n",
      "\n",
      "\n",
      "\n",
      "LG그룹, AI 토크 콘서트서 '엑사원 생태계' 공개\n",
      "\n",
      "\n",
      "\n",
      "네이버, 하이퍼클로바X 추론 모델도 상업용 무료 공개\n",
      "\n",
      "\n",
      "\n",
      "극한폭우 8월서 7월로 당겨진다…온난화 영향 ‘기압계’ 변화\n",
      "\n",
      "\n",
      "\n",
      "하루 100번, 10년도 거뜬 삼성D, 폴더블 내구성 입증\n",
      "\n",
      "\n",
      "\n",
      "NASA의 조용한 초음속 비행기, 언제 하늘 날까\n",
      "\n",
      "\n",
      "\n",
      "이통3사 갤Z폴드7·플립7 사전개통 시작…카드 할인 등 혜택\n",
      "\n",
      "\n",
      "\n",
      "네이버클라우드, AI 추론 모델 '하이퍼클로바X 시드 14B 씽크' 상업용 무료 공개\n",
      "\n",
      "\n",
      "\n",
      "\"상업용도 공짜!\"…네이버, '하이퍼클로바 X 시드 14B 씽크' 오픈소스 공개\n",
      "\n",
      "\n",
      "\n",
      "'이틀 만에 21만대' 반응 폭발…삼성, 인도서 결국 일냈다\n",
      "\n",
      "\n",
      "\n",
      "좌초하고 있는 트럼프 행정부 야심찬 AI 프로젝트 '스타게이트'\n",
      "\n",
      "\n",
      "\n",
      "[단독] “아직도 윤석열 정부?”…‘AI’ 구글 제미나이, 현실 인식 오류 반복 중\n",
      "\n",
      "\n",
      "\n",
      "“아들은 만들고 엄마는 배달” 무서운 가족…불법 ‘몸짱약’ 12억원어치나 팔아치웠다\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# IT/과학 뉴스 \n",
    "req_param = {\n",
    "    'sid': 105\n",
    "}\n",
    "# \n",
    "url = 'https://news.naver.com/section/{sid}'.format(**req_param)\n",
    "print(url)\n",
    "\n",
    "# 요청 헤더 설정 : 브라우저 정보\n",
    "req_header = {\n",
    "    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/58.0.3029.110 Safari/537.3'\n",
    "\n",
    "}\n",
    "\n",
    "\n",
    "# requests 의 get() 함수 호출하기 \n",
    "response = requests.get(url, headers=req_header)\n",
    "# 응답(response)이 OK 이면\n",
    "# 응답 (response)에서 text 추출\n",
    "if response.ok:\n",
    "    # 응답(response)에서 text 추출\n",
    "    html = response.text\n",
    "    # BeautifulSoup 객체 생성\n",
    "    soup = BeautifulSoup(html, 'html.parser')\n",
    "    # CSS 선택자\n",
    "    ##print(soup.select(\"div.sa_text a[href*='mnews/article']\"))\n",
    "\n",
    "# BeautifulSoup 객체 생성\n",
    "soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "# CSS 선택자\n",
    "title = soup.select(\"div.sa_text a[href*='mnews/article']\")\n",
    "for t in title:\n",
    "    # a 태그의 text 출력\n",
    "    print(t.text)\n",
    "    # # a 태그의 href 속성값 출력\n",
    "    # print(t['href'])\n",
    "\n",
    "# print(soup.select(\"div.sa_text a[href*='mnews/article']\"))\n",
    "\n",
    "# <a> 태그 리스트 순회하기    \n",
    "\n",
    "# 응답(response)이 Error 이면 status code 출력    \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45640860-a449-4285-90d7-5f300292b461",
   "metadata": {},
   "source": [
    "### 1.1 뉴스제목 추출하는 함수 선언하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f2e4ef8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "section_dict = {100:'정치',101:'경제',102:'사회',103:'생활/문화',104:'세계',105:'IT/과학'}\n",
    "\n",
    "def print_news(sid, section):  #print_new(103,'생활/문화')\n",
    "\n",
    "    title_text = []\n",
    "    req_param = {\n",
    "        'sid': sid\n",
    "    }\n",
    "    url = 'https://news.naver.com/section/{sid}'.format(**req_param)\n",
    "    print(f\"URL: {url} ({section})\")\n",
    "    req_header = {\n",
    "        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/58.0.3029.110 Safari/537.3'\n",
    "    }\n",
    "    response = requests.get(url, headers=req_header)\n",
    "    \n",
    "\n",
    "    if response.ok:\n",
    "        # 응답(response)에서 text 추출\n",
    "        print(response.status_code)    \n",
    "\n",
    "        soup = BeautifulSoup(response.text, 'html.parser')\n",
    "          \n",
    "        title = soup.select(\"div.sa_text a[href*='news/article/'].sa_text_title\" )\n",
    "        \n",
    "        print(type(title))\n",
    "        print(f\"뉴스 제목 개수: {len(title)}\")\n",
    "           \n",
    "        for idx, t in enumerate(title,1):\n",
    "            #strong_tag = title.find('strong').strip()\n",
    "            title_text.append(t.text.strip())\n",
    "            link = t['href'] if 'href' in t.attrs else 'No link'            \n",
    "            print(f\"{idx}, {t.text.strip()}, {link}\")\n",
    "     \n",
    "\n",
    "\n",
    "    else:\n",
    "        print(f\"Error: {response.status_code}\")\n",
    "\n",
    "    return\n",
    "    #return title_text\n",
    "print_news(section_var,section_dict[section_var])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "858952c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "section_var = 104\n",
    "\n",
    "\n",
    "section_dict2 = {'정치':100, '경제':101, '사회':102, '생활/문화':103, '세계':104, 'IT/과학':105}\n",
    "def print_news_section(section):\n",
    "    sid = section_dict2[section]\n",
    "    url = f'https://news.naver.com/section/{sid}'\n",
    "    #section = section_dict[sid]\n",
    "    print(f\"URL: {url} ({section} 뉴스)\")\n",
    "    \n",
    "    req_header = {\n",
    "        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/58.0.3029.110 Safari/537.3'\n",
    "    }\n",
    "    res = requests.get(url, headers=req_header)\n",
    "    print(res.status_code,res.ok)\n",
    "    if res.ok:\n",
    "        soup = BeautifulSoup(res.text, 'html.parser')\n",
    "        a_tags = soup.select(\"div.sa_text a[href*='news/article/'].sa_text_title\")\n",
    "        print(len(a_tags))\n",
    "    else:\n",
    "        print(f\"Error: {res.status_code}\")\n",
    "    for idx, a in enumerate(a_tags, 1):\n",
    "        if a.text:\n",
    "            title = a.text.strip()\n",
    "            link = a['href'] if 'href' in a.attrs else 'No link'\n",
    "            print(f\"{idx}, {title}, {link}\")\n",
    "\n",
    "print_news_section('세계')\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "323196dc",
   "metadata": {},
   "source": [
    "### 2. Image 다운로드\n",
    "* referer 요청 헤더를 반드시 설정해야 한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "id": "c72d6cb1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved 20250526143831_077fd585b6c39ba304bb6f45ca4d3e30_IMAG01_13.jpg successfully. (126,059) bytes\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import os\n",
    "\n",
    "req_header = {\n",
    "    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/58.0.3029.110 Safari/537.3',\n",
    "    'referer' : \"https://comic.naver.com/webtoon/detail?titleId=819929&no=83&week=tue\",\n",
    "}\n",
    "\n",
    "\n",
    "\n",
    "img_urls = [\n",
    "    \"https://image-comic.pstatic.net/webtoon/819929/83/20250526143831_077fd585b6c39ba304bb6f45ca4d3e30_IMAG01_13.jpg\"\n",
    "\n",
    "]\n",
    "\n",
    "for img_url in img_urls:\n",
    "    pass\n",
    "    # requests 의 get(url, headers) 함수 호출하기\n",
    "    response = requests.get(img_url, headers=req_header)\n",
    "        \n",
    "        # binary 응답 데이터 가져오기\n",
    "    if response.ok:\n",
    "        binary_data = response.content\n",
    "        # url에서 파일명만 추출하기\n",
    "        file_name = os.path.basename(img_url)\n",
    "        # binday data를 file에 write하기\n",
    "        with open(file_name, 'wb') as f:\n",
    "            f.write(binary_data)\n",
    "            print(f\"Saved {file_name} successfully. ({(len(binary_data)):,}) bytes\")\n",
    "\n",
    "    else:\n",
    "        print(f\"Failed to download {img_url}. Status code: {response.status_code}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5375653",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e8b00a04",
   "metadata": {},
   "source": [
    "* 현재 요청된 페이지의 image 모두 다운로드 해보기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b30f7bf5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "URL: https://comic.naver.com/webtoon/detail?titleId=819929&no=83)\n",
      "True\n",
      "79\n"
     ]
    }
   ],
   "source": [
    "import os, requests\n",
    "from bs4 import BeautifulSoup\n",
    "def a ():\n",
    "    url = 'https://comic.naver.com/webtoon/detail?titleId=819929&no=83'\n",
    "    print(f\"URL: {url})\")\n",
    "    req_header = {\n",
    "        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/58.0.3029.110 Safari/537.3',\n",
    "        'referer': url,\n",
    "    }\n",
    "    res = requests.get(url,headers=req_header)\n",
    "    print(res.ok)\n",
    "\n",
    "    if res.ok:\n",
    "        soup = BeautifulSoup(res.text,'html.parser')\n",
    "        #title = soup.select(\"div.sa_text a[href*='news/article/'].sa_text_title\" )\n",
    "        print (len(soup.select(\"div.wt_viewer img[src^='https://image-comic'] \")))\n",
    "\n",
    "        \n",
    "\n",
    "        pass\n",
    "\n",
    "\n",
    "a()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ac9f5f9",
   "metadata": {},
   "source": [
    "### 3. 파일 업로드 하기\n",
    "* http://httpbin.org/post 업로드 요청을 할 수 있는 url"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04da4790",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "200\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "\n",
    "upload_files = {\n",
    "    \n",
    "}\n",
    "#print(upload_files)\n",
    "\n",
    "url = 'http://httpbin.org/post'\n",
    "# file 업로드 하려면 requests의 post 함수에 files 속성을 사용합니다.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ad65f12",
   "metadata": {},
   "source": [
    "### 4. 캡챠(이미지) API 호출하기\n",
    "* urllib 사용\n",
    "* 1. 캡차 키 발급 요청\n",
    "  2. 캡차 이미지 요청\n",
    "  3. 사용자 입력값 검증 요청"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc4ae360-e772-4873-8642-d3494edd34e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 캡차 키 발급 요청\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b95b6a0-c218-4792-82a6-da4d80872071",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 캡차 이미지 요청\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a79076af-2cd0-4de2-8301-316b9130c55e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#  사용자 입력값 검증 요청\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7d984a6",
   "metadata": {},
   "source": [
    "* requests를 사용하는 코드로 변경하기\n",
    "* [requests docs](https://requests.readthedocs.io/en/latest/user/quickstart/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "727b427b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error Code: 403\n"
     ]
    }
   ],
   "source": [
    "# 사용자 입력값 검증 요청\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5e1179a",
   "metadata": {},
   "source": [
    "### 5. 블로그 검색하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "877d0c7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import pprint\n",
    "\n",
    "headers = {\n",
    "    'X-Naver-Client-Id': '',\n",
    "    'X-Naver-Client-Secret': '',\n",
    "}\n",
    "\n",
    "payload = {\n",
    "    'query': '파이썬',\n",
    "    'display': 100,\n",
    "    'sort': 'sim'\n",
    "}\n",
    "\n",
    "url = 'https://openapi.naver.com/v1/search/blog.json'\n",
    "\n",
    "\n",
    "# requests get(url, params, headers) 요청 \n",
    "\n",
    "# json() 함수로 응답 결과 가져오오기\n",
    "# 'title' , 'bloggername' , 'description' , 'bloggerlink' , 'link'\n",
    "\n",
    "# 'data/nhnblog.txt' 파일 생성하기\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
